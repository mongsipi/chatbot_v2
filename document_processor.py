# document_processor.py - ì•ˆì •í™”ëœ ë‹¤ì¤‘ ë¬¸ì„œ í˜•ì‹ ì²˜ë¦¬ ë° ë²¡í„° ê²€ìƒ‰ ëª¨ë“ˆ
import os
import json
import pickle
import re
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# PDF ì²˜ë¦¬
import PyPDF2
import pdfplumber

# Office ë¬¸ì„œ ì²˜ë¦¬ (ì•ˆì „í•œ ë¡œë”©)
try:
    from docx import Document
    HAS_DOCX = True
    print("âœ“ python-docx ë¡œë”© ì„±ê³µ")
except ImportError as e:
    HAS_DOCX = False
    print(f"âš ï¸ python-docx ì—†ìŒ: Word íŒŒì¼ ì§€ì› ì•ˆí•¨ - {e}")

try:
    from pptx import Presentation
    HAS_PPTX = True
    print("âœ“ python-pptx ë¡œë”© ì„±ê³µ")
except ImportError as e:
    HAS_PPTX = False
    print(f"âš ï¸ python-pptx ì—†ìŒ: PowerPoint íŒŒì¼ ì§€ì› ì•ˆí•¨ - {e}")

try:
    import openpyxl
    import pandas as pd
    HAS_EXCEL = True
    print("âœ“ Excel ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì„±ê³µ")
except ImportError as e:
    HAS_EXCEL = False
    print(f"âš ï¸ openpyxl/pandas ì—†ìŒ: Excel íŒŒì¼ ì§€ì› ì•ˆí•¨ - {e}")

# sentence-transformers (ì•ˆì „í•œ ë¡œë”©)
try:
    from sentence_transformers import SentenceTransformer
    HAS_SENTENCE_TRANSFORMERS = True
    print("âœ“ sentence-transformers ë¡œë”© ì„±ê³µ")
except ImportError as e:
    HAS_SENTENCE_TRANSFORMERS = False
    print(f"âš ï¸ sentence-transformers ì—†ìŒ: ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš© - {e}")

class SimpleEmbedding:
    """sentence-transformersê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ ì„ë² ë”©"""
    
    def __init__(self):
        self.vocabulary = {}
        self.vocab_size = 0
        print("SimpleEmbedding ì´ˆê¸°í™” (fallback ëª¨ë“œ)")
    
    def _build_vocabulary(self, texts):
        """í…ìŠ¤íŠ¸ì—ì„œ ì–´íœ˜ êµ¬ì¶•"""
        words = set()
        for text in texts:
            if isinstance(text, str):
                words.update(text.lower().split())
        
        self.vocabulary = {word: idx for idx, word in enumerate(sorted(words))}
        self.vocab_size = len(self.vocabulary)
    
    def encode(self, texts):
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if isinstance(texts, str):
            texts = [texts]
        
        if not self.vocabulary:
            self._build_vocabulary(texts)
        
        vectors = []
        for text in texts:
            vector = np.zeros(self.vocab_size)
            if isinstance(text, str):
                words = text.lower().split()
                for word in words:
                    if word in self.vocabulary:
                        vector[self.vocabulary[word]] += 1
            
            # ì •ê·œí™”
            if np.linalg.norm(vector) > 0:
                vector = vector / np.linalg.norm(vector)
            
            vectors.append(vector)
        
        return np.array(vectors)

class DocumentProcessor:
    """ì•ˆì •í™”ëœ ë‹¤ì¤‘ ë¬¸ì„œ í˜•ì‹ ì²˜ë¦¬ ë° ë²¡í„° ê²€ìƒ‰ í´ë˜ìŠ¤"""
    
    def __init__(self, upload_folder):
        print(f"\n=== DocumentProcessor ì´ˆê¸°í™” ì‹œì‘ ===")
        
        self.upload_folder = upload_folder
        self.embeddings_file = os.path.join(upload_folder, 'embeddings.pkl')
        self.metadata_file = os.path.join(upload_folder, 'metadata.json')
        
        # ì‹¤ì œ ì§€ì› ê°€ëŠ¥í•œ íŒŒì¼ í™•ì¥ìë§Œ í¬í•¨
        self.supported_extensions = {'.pdf': 'PDF'}
        
        if HAS_DOCX:
            self.supported_extensions['.docx'] = 'Word'
        if HAS_PPTX:
            self.supported_extensions['.pptx'] = 'PowerPoint'
        if HAS_EXCEL:
            self.supported_extensions['.xlsx'] = 'Excel'
            self.supported_extensions['.xls'] = 'Excel (Legacy)'
        
        print(f"ì§€ì› íŒŒì¼ í˜•ì‹: {list(self.supported_extensions.values())}")
        
        # ì„ë² ë”© ëª¨ë¸ ì•ˆì „ ì´ˆê¸°í™”
        self.encoder = None
        self._safe_init_encoder()
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.documents = []
        self.embeddings = None
        self.metadata = {}
        
        # ë°ì´í„° ë¡œë“œ
        self.load_data()
        
        print(f"=== DocumentProcessor ì´ˆê¸°í™” ì™„ë£Œ ===")
        print(f"ë¡œë“œëœ ë¬¸ì„œ: {len(self.documents)}ê°œ")
    
    def _safe_init_encoder(self):
        """ì„ë² ë”© ëª¨ë¸ ì•ˆì „ ì´ˆê¸°í™”"""
        if HAS_SENTENCE_TRANSFORMERS:
            try:
                print("sentence-transformers ëª¨ë¸ ë¡œë”© ì‹œë„...")
                # ê°€ì¥ ì•ˆì •ì ì¸ ëª¨ë¸ ìˆœì„œëŒ€ë¡œ ì‹œë„
                models_to_try = [
                    'all-MiniLM-L6-v2',
                    'paraphrase-MiniLM-L6-v2', 
                    'all-mpnet-base-v2'
                ]
                
                for model_name in models_to_try:
                    try:
                        print(f"ëª¨ë¸ ì‹œë„: {model_name}")
                        self.encoder = SentenceTransformer(model_name)
                        print(f"âœ“ {model_name} ë¡œë”© ì„±ê³µ")
                        return
                    except Exception as e:
                        print(f"âœ— {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
                        continue
                
                print("ëª¨ë“  sentence-transformers ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, SimpleEmbedding ì‚¬ìš©")
                self.encoder = SimpleEmbedding()
                
            except Exception as e:
                print(f"sentence-transformers ì´ˆê¸°í™” ì „ì²´ ì‹¤íŒ¨: {e}")
                self.encoder = SimpleEmbedding()
        else:
            print("sentence-transformers ì—†ìŒ, SimpleEmbedding ì‚¬ìš©")
            self.encoder = SimpleEmbedding()
    
    def extract_text_from_pdf(self, pdf_path):
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ˆì •í™”)"""
        print(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ: {os.path.basename(pdf_path)}")
        text = ""
        
        try:
            # pdfplumber ìš°ì„  ì‹œë„
            with pdfplumber.open(pdf_path) as pdf:
                for i, page in enumerate(pdf.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text and page_text.strip():
                            text += page_text + "\n"
                    except Exception as e:
                        print(f"í˜ì´ì§€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue
                        
        except Exception as e1:
            print(f"pdfplumber ì‹¤íŒ¨, PyPDF2 ì‹œë„: {e1}")
            
            try:
                with open(pdf_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    for i, page in enumerate(pdf_reader.pages):
                        try:
                            page_text = page.extract_text()
                            if page_text and page_text.strip():
                                text += page_text + "\n"
                        except Exception as e:
                            print(f"PyPDF2 í˜ì´ì§€ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                            continue
            except Exception as e2:
                print(f"PyPDF2ë„ ì‹¤íŒ¨: {e2}")
                return None
        
        extracted_text = text.strip() if text.strip() else None
        if extracted_text:
            print(f"âœ“ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(extracted_text)}ì")
        else:
            print("âœ— PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
        
        return extracted_text
    
    def extract_text_from_docx(self, docx_path):
        """Word ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ˆì „í™”)"""
        if not HAS_DOCX:
            return None
        
        try:
            print(f"Word ë¬¸ì„œ ì²˜ë¦¬: {os.path.basename(docx_path)}")
            doc = Document(docx_path)
            text = ""
            
            # ë‹¨ë½ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for paragraph in doc.paragraphs:
                if paragraph.text and paragraph.text.strip():
                    text += paragraph.text.strip() + "\n"
            
            # í‘œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
            try:
                for table in doc.tables:
                    for row in table.rows:
                        row_text = []
                        for cell in row.cells:
                            if cell.text and cell.text.strip():
                                row_text.append(cell.text.strip())
                        if row_text:
                            text += " | ".join(row_text) + "\n"
            except Exception as e:
                print(f"Word í‘œ ì²˜ë¦¬ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
            
            result = text.strip() if text.strip() else None
            if result:
                print(f"âœ“ Word í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(result)}ì")
            return result
            
        except Exception as e:
            print(f"âœ— Word ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None
    
    def extract_text_from_pptx(self, pptx_path):
        """PowerPoint ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ˆì „í™”)"""
        if not HAS_PPTX:
            return None
        
        try:
            print(f"PowerPoint ë¬¸ì„œ ì²˜ë¦¬: {os.path.basename(pptx_path)}")
            prs = Presentation(pptx_path)
            text = ""
            
            for i, slide in enumerate(prs.slides):
                slide_text = f"\n=== ìŠ¬ë¼ì´ë“œ {i+1} ===\n"
                
                # í…ìŠ¤íŠ¸ ìƒìì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                for shape in slide.shapes:
                    try:
                        if hasattr(shape, "text") and shape.text and shape.text.strip():
                            slide_text += shape.text.strip() + "\n"
                        
                        # í‘œ ì²˜ë¦¬ (ì•ˆì „í•˜ê²Œ)
                        if hasattr(shape, 'has_table') and shape.has_table:
                            try:
                                table = shape.table
                                for row in table.rows:
                                    row_text = []
                                    for cell in row.cells:
                                        if cell.text and cell.text.strip():
                                            row_text.append(cell.text.strip())
                                    if row_text:
                                        slide_text += " | ".join(row_text) + "\n"
                            except Exception as e:
                                print(f"PowerPoint í‘œ ì²˜ë¦¬ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
                                
                    except Exception as e:
                        print(f"PowerPoint shape ì²˜ë¦¬ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
                        continue
                
                if slide_text.strip() != f"=== ìŠ¬ë¼ì´ë“œ {i+1} ===":
                    text += slide_text
            
            result = text.strip() if text.strip() else None
            if result:
                print(f"âœ“ PowerPoint í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(result)}ì")
            return result
            
        except Exception as e:
            print(f"âœ— PowerPoint ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None
    
    def extract_text_from_excel(self, excel_path):
        """Excel ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ˆì „í™”)"""
        if not HAS_EXCEL:
            return None
        
        try:
            print(f"Excel ë¬¸ì„œ ì²˜ë¦¬: {os.path.basename(excel_path)}")
            text = ""
            
            # pandasë¡œ ì•ˆì „í•˜ê²Œ ì½ê¸°
            excel_file = pd.ExcelFile(excel_path)
            
            for sheet_name in excel_file.sheet_names:
                try:
                    df = pd.read_excel(excel_path, sheet_name=sheet_name)
                    
                    if not df.empty:
                        sheet_text = f"\n=== {sheet_name} ì‹œíŠ¸ ===\n"
                        
                        # ì»¬ëŸ¼ëª… ì¶”ê°€
                        if not df.columns.empty:
                            valid_columns = [str(col) for col in df.columns if str(col).strip()]
                            if valid_columns:
                                sheet_text += "ì»¬ëŸ¼: " + " | ".join(valid_columns) + "\n"
                        
                        # ë°ì´í„° ì¶”ê°€ (í…ìŠ¤íŠ¸ë§Œ)
                        for index, row in df.iterrows():
                            row_text = []
                            for value in row:
                                if pd.notna(value) and str(value).strip():
                                    str_value = str(value).strip()
                                    # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ í¬í•¨ (ìˆ«ìë§Œ ìˆëŠ” ê²ƒ ì œì™¸)
                                    if len(str_value) > 1 and not str_value.replace('.', '').replace('-', '').replace(',', '').isdigit():
                                        row_text.append(str_value)
                            
                            if row_text:
                                sheet_text += " | ".join(row_text) + "\n"
                        
                        text += sheet_text
                        
                except Exception as e:
                    print(f"Excel ì‹œíŠ¸ {sheet_name} ì²˜ë¦¬ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
                    continue
            
            result = text.strip() if text.strip() else None
            if result:
                print(f"âœ“ Excel í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(result)}ì")
            return result
            
        except Exception as e:
            print(f"âœ— Excel ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None
    
    def extract_text_from_document(self, file_path):
        """íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ì ì ˆí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²• ì„ íƒ"""
        filename = os.path.basename(file_path)
        _, ext = os.path.splitext(filename.lower())
        
        if ext == '.pdf':
            return self.extract_text_from_pdf(file_path)
        elif ext == '.docx':
            return self.extract_text_from_docx(file_path)
        elif ext == '.pptx':
            return self.extract_text_from_pptx(file_path)
        elif ext in ['.xlsx', '.xls']:
            return self.extract_text_from_excel(file_path)
        else:
            print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")
            return None
    
    def clean_text(self, text, file_type=None):
        """í…ìŠ¤íŠ¸ ì •ë¦¬ (ê°„ì†Œí™” ë° ì•ˆì „í™”)"""
        if not text or not isinstance(text, str):
            return ""
        
        try:
            # ê¸°ë³¸ ì •ë¦¬ë§Œ ìˆ˜í–‰ (ë³µì¡í•œ ì •ê·œì‹ ìµœì†Œí™”)
            # 1. ì œì–´ë¬¸ì ì œê±°
            text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
            
            # 2. ê¸°ë³¸ íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
            text = text.replace('Â·', ' ')
            text = text.replace('ã†', ' ')
            
            # 3. ê³¼ë„í•œ ê³µë°± ì •ë¦¬
            text = re.sub(r'\s+', ' ', text)
            text = re.sub(r'\n\s*\n+', '\n', text)
            
            # 4. ê¸°ë³¸ ì¤„ í•„í„°ë§
            lines = []
            for line in text.split('\n'):
                line = line.strip()
                if line and len(line) > 2:  # ë„ˆë¬´ ì§§ì€ ì¤„ ì œì™¸
                    lines.append(line)
            
            cleaned_text = '\n'.join(lines)
            return cleaned_text
            
        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ì •ë¦¬ ì˜¤ë¥˜: {e}")
            return text  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë°˜í™˜
    
    def chunk_text(self, text, chunk_size=600, overlap=50):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ì•ˆì „í™”)"""
        if not text:
            return []
        
        try:
            # ê°„ë‹¨í•œ ì²­í‚¹ ë°©ì‹ ì‚¬ìš©
            chunks = []
            
            # ë¬¸ë‹¨ë³„ë¡œ ë¶„í• 
            paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
            
            current_chunk = ""
            for paragraph in paragraphs:
                # í˜„ì¬ ì²­í¬ì— ì¶”ê°€ ê°€ëŠ¥í•œì§€ í™•ì¸
                if len(current_chunk + paragraph) <= chunk_size:
                    current_chunk += paragraph + "\n"
                else:
                    # í˜„ì¬ ì²­í¬ ì €ì¥
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    
                    # ìƒˆ ì²­í¬ ì‹œì‘
                    if len(paragraph) > chunk_size:
                        # ë„ˆë¬´ ê¸´ ë¬¸ë‹¨ì€ ê°•ì œ ë¶„í• 
                        words = paragraph.split()
                        temp_chunk = ""
                        for word in words:
                            if len(temp_chunk + word) <= chunk_size:
                                temp_chunk += word + " "
                            else:
                                if temp_chunk.strip():
                                    chunks.append(temp_chunk.strip())
                                temp_chunk = word + " "
                        current_chunk = temp_chunk
                    else:
                        current_chunk = paragraph + "\n"
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            
            # ë¹ˆ ì²­í¬ ì œê±°
            chunks = [chunk for chunk in chunks if chunk.strip()]
            
            return chunks if chunks else [text[:chunk_size]]
            
        except Exception as e:
            print(f"ì²­í‚¹ ì˜¤ë¥˜: {e}")
            return [text]  # ì˜¤ë¥˜ ì‹œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
    
    def process_document(self, file_path):
        """ë¬¸ì„œ íŒŒì¼ ì²˜ë¦¬ (ì•ˆì „í™”)"""
        try:
            filename = os.path.basename(file_path)
            _, ext = os.path.splitext(filename.lower())
            
            print(f"\n=== ë¬¸ì„œ ì²˜ë¦¬: {filename} ===")
            
            # ì§€ì› í˜•ì‹ í™•ì¸
            if ext not in self.supported_extensions:
                print(f"âœ— ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")
                return False
            
            file_type = self.supported_extensions[ext]
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = self.extract_text_from_document(file_path)
            if not text:
                print(f"âœ— í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {filename}")
                return False
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            cleaned_text = self.clean_text(text, file_type)
            if len(cleaned_text) < 50:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                print(f"âœ— í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ: {filename} ({len(cleaned_text)}ì)")
                return False
            
            # ì²­í‚¹
            chunks = self.chunk_text(cleaned_text)
            if not chunks:
                print(f"âœ— ì²­í¬ ìƒì„± ì‹¤íŒ¨: {filename}")
                return False
            
            # ê¸°ì¡´ ë¬¸ì„œê°€ ìˆë‹¤ë©´ ì œê±°
            self.remove_document_by_filename(filename)
            
            # ë¬¸ì„œ ì¶”ê°€
            for i, chunk in enumerate(chunks):
                doc = {
                    'content': chunk,
                    'filename': filename,
                    'file_type': file_type,
                    'chunk_id': i,
                    'file_path': file_path,
                    'processed_date': datetime.now().isoformat()
                }
                self.documents.append(doc)
            
            # ì„ë² ë”© ìƒì„±
            self.update_embeddings()
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self.metadata[filename] = {
                'file_path': file_path,
                'file_type': file_type,
                'chunks_count': len(chunks),
                'processed_date': datetime.now().isoformat(),
                'file_size': os.path.getsize(file_path)
            }
            
            # ë°ì´í„° ì €ì¥
            self.save_data()
            
            print(f"âœ“ ì²˜ë¦¬ ì™„ë£Œ: {filename} ({file_type}, {len(chunks)} ì²­í¬)")
            return True
            
        except Exception as e:
            print(f"âœ— ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def update_embeddings(self):
        """ë¬¸ì„œë“¤ì˜ ì„ë² ë”© ì—…ë°ì´íŠ¸ (ì•ˆì „í™”)"""
        if not self.documents:
            self.embeddings = None
            return
        
        try:
            contents = [doc['content'] for doc in self.documents if doc.get('content')]
            if contents and self.encoder:
                self.embeddings = self.encoder.encode(contents)
                print(f"âœ“ ì„ë² ë”© ì—…ë°ì´íŠ¸: {len(contents)} ë¬¸ì„œ")
            else:
                print("âš ï¸ ì„ë² ë”© ìƒì„± ìŠ¤í‚µ (ë‚´ìš© ì—†ìŒ ë˜ëŠ” ì¸ì½”ë” ì—†ìŒ)")
                self.embeddings = None
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            self.embeddings = None
    
    def search_similar_documents(self, query, top_k=5, min_similarity=0.01):
        """ë‹¤ì¤‘ ê²€ìƒ‰ ë°©ë²•ì„ ì‚¬ìš©í•œ ë¬¸ì„œ ê²€ìƒ‰ (ì•ˆì „í™”)"""
        if not self.documents:
            return []
        
        print(f"\n=== ê²€ìƒ‰: '{query}' ===")
        
        all_results = []
        
        # 1. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (ê°€ì¥ ì•ˆì •ì )
        keyword_results = self.keyword_search(query, top_k)
        all_results.extend(keyword_results)
        
        # 2. ë²¡í„° ì„ë² ë”© ê²€ìƒ‰ (ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if self.embeddings is not None:
            try:
                vector_results = self.vector_search(query, top_k, min_similarity)
                all_results.extend(vector_results)
            except Exception as e:
                print(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
        
        # 3. ë¶€ë¶„ ë¬¸ìì—´ ê²€ìƒ‰
        substring_results = self.substring_search(query, top_k)
        all_results.extend(substring_results)
        
        # 4. ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
        final_results = self.merge_and_rank_results(all_results, query)
        
        print(f"ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼")
        return final_results[:top_k]
    
    def keyword_search(self, query, top_k=5):
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (í•µì‹¬ ê¸°ëŠ¥)"""
        keywords = self.extract_keywords(query)
        results = []
        
        for doc in self.documents:
            score = 0
            content_lower = doc['content'].lower()
            
            for keyword in keywords:
                keyword_lower = keyword.lower()
                count = content_lower.count(keyword_lower)
                if count > 0:
                    score += count * len(keyword)
            
            if score > 0:
                results.append({
                    'document': doc,
                    'similarity': min(score / 100.0, 1.0),
                    'content': doc['content'],
                    'method': 'keyword'
                })
        
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]
    
    def vector_search(self, query, top_k=5, min_similarity=0.01):
        """ë²¡í„° ì„ë² ë”© ê²€ìƒ‰"""
        try:
            query_embedding = self.encoder.encode([query])
            similarities = cosine_similarity(query_embedding, self.embeddings)[0]
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                similarity = similarities[idx]
                if similarity >= min_similarity:
                    results.append({
                        'document': self.documents[idx],
                        'similarity': float(similarity),
                        'content': self.documents[idx]['content'],
                        'method': 'vector'
                    })
            
            return results
        except Exception as e:
            print(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def substring_search(self, query, top_k=5):
        """ë¶€ë¶„ ë¬¸ìì—´ ê²€ìƒ‰"""
        results = []
        query_lower = query.lower()
        
        for doc in self.documents:
            content_lower = doc['content'].lower()
            
            if query_lower in content_lower:
                similarity = len(query) / len(doc['content'])
                results.append({
                    'document': doc,
                    'similarity': min(similarity * 10, 1.0),
                    'content': doc['content'],
                    'method': 'substring'
                })
        
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]
    
    def extract_keywords(self, query):
        """ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ì™€', 'ê³¼', 'ì˜', 'ë¡œ', 'ìœ¼ë¡œ', 
                    'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ë¶€í„°', 'ê¹Œì§€', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ê°™ì´', 'ì™€', 'ê³¼', 
                    'ê·¸ë¦¬ê³ ', 'ë˜ëŠ”', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë¬´ì—‡', 'ì–´ë–¤', 'ì–´ë””', 'ì–¸ì œ', 'ì™œ', 'ì–´ë–»ê²Œ',
                    'ë­', 'ë­”', 'ë­˜', 'ë­ê°€', 'ë­ë¥¼', 'ì…ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ë‹¤', 'ìš”', 'ì£ ', 'ìš”?', 'ê¹Œìš”?']
        
        words = re.findall(r'[ê°€-í£]+|[a-zA-Z]+|\d+', query)
        keywords = [word for word in words if len(word) >= 2 and word not in stopwords]
        
        return keywords
    
    def merge_and_rank_results(self, all_results, query):
        """ê²€ìƒ‰ ê²°ê³¼ í†µí•© ë° ìˆœìœ„ ê²°ì •"""
        if not all_results:
            return []
        
        doc_scores = {}
        
        for result in all_results:
            doc_id = id(result['document'])
            
            if doc_id not in doc_scores:
                doc_scores[doc_id] = {
                    'document': result['document'],
                    'content': result['content'],
                    'scores': [],
                    'methods': []
                }
            
            doc_scores[doc_id]['scores'].append(result['similarity'])
            doc_scores[doc_id]['methods'].append(result['method'])
        
        final_results = []
        for doc_id, info in doc_scores.items():
            if len(info['scores']) > 1:
                final_score = max(info['scores']) * 0.7 + sum(info['scores']) / len(info['scores']) * 0.3
            else:
                final_score = info['scores'][0]
            
            if 'keyword' in info['methods']:
                final_score *= 1.5
            
            final_results.append({
                'document': info['document'],
                'content': info['content'],
                'similarity': min(final_score, 1.0),
                'methods': info['methods']
            })
        
        final_results.sort(key=lambda x: x['similarity'], reverse=True)
        return final_results
    
    # ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ (ì›ë³¸ê³¼ ë™ì¼í•˜ì§€ë§Œ ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”)
    def remove_document_by_filename(self, filename):
        """íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ë¬¸ì„œ ì œê±°"""
        self.documents = [doc for doc in self.documents if doc['filename'] != filename]
        if filename in self.metadata:
            del self.metadata[filename]
    
    def save_data(self):
        """ë°ì´í„° ì €ì¥"""
        try:
            if self.embeddings is not None:
                with open(self.embeddings_file, 'wb') as f:
                    pickle.dump({
                        'embeddings': self.embeddings,
                        'documents': self.documents
                    }, f)
            
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        try:
            if os.path.exists(self.embeddings_file):
                with open(self.embeddings_file, 'rb') as f:
                    data = pickle.load(f)
                    self.embeddings = data.get('embeddings')
                    self.documents = data.get('documents', [])
            
            if os.path.exists(self.metadata_file):
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                    
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.documents = []
            self.embeddings = None
            self.metadata = {}
    
    def get_uploaded_files(self):
        """ì—…ë¡œë“œëœ ë¬¸ì„œ íŒŒì¼ ëª©ë¡"""
        try:
            files = []
            for filename in os.listdir(self.upload_folder):
                _, ext = os.path.splitext(filename.lower())
                if ext in self.supported_extensions:
                    filepath = os.path.join(self.upload_folder, filename)
                    files.append({
                        'filename': filename,
                        'file_type': self.supported_extensions[ext],
                        'size': os.path.getsize(filepath),
                        'modified': datetime.fromtimestamp(os.path.getmtime(filepath)).isoformat()
                    })
            return files
        except Exception as e:
            print(f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def get_processed_files_info(self):
        """ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´"""
        return [
            {
                'filename': filename,
                'file_type': info['file_type'],
                'chunks_count': info['chunks_count'],
                'processed_date': info['processed_date'],
                'file_size': info.get('file_size', 0)
            }
            for filename, info in self.metadata.items()
        ]
    
    def has_processed_documents(self):
        """ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸"""
        return len(self.documents) > 0
    
    def delete_file(self, filename):
        """íŒŒì¼ ì‚­ì œ"""
        try:
            filepath = os.path.join(self.upload_folder, filename)
            if os.path.exists(filepath):
                os.remove(filepath)
            
            self.remove_document_by_filename(filename)
            self.update_embeddings()
            self.save_data()
            
            return True
        except Exception as e:
            print(f"íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: {e}")
            return False
    
    def reprocess_all_documents(self):
        """ëª¨ë“  ë¬¸ì„œ íŒŒì¼ ì¬ì²˜ë¦¬"""
        try:
            self.documents = []
            self.embeddings = None
            self.metadata = {}
            
            success_count = 0
            document_files = []
            
            for filename in os.listdir(self.upload_folder):
                _, ext = os.path.splitext(filename.lower())
                if ext in self.supported_extensions:
                    document_files.append(filename)
            
            for filename in document_files:
                filepath = os.path.join(self.upload_folder, filename)
                if self.process_document(filepath):
                    success_count += 1
            
            return success_count == len(document_files)
            
        except Exception as e:
            print(f"ì¬ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def initialize_existing_documents(self):
        """ì„œë²„ ì‹œì‘ì‹œ ê¸°ì¡´ ë¬¸ì„œ íŒŒì¼ë“¤ ì²˜ë¦¬"""
        try:
            for filename in os.listdir(self.upload_folder):
                _, ext = os.path.splitext(filename.lower())
                if ext in self.supported_extensions:
                    if filename not in self.metadata:
                        filepath = os.path.join(self.upload_folder, filename)
                        print(f"ê¸°ì¡´ íŒŒì¼ ì²˜ë¦¬: {filename}")
                        self.process_document(filepath)
                        
        except Exception as e:
            print(f"ê¸°ì¡´ íŒŒì¼ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")


class QuestionAnalyzer:
    """ì§ˆë¬¸ ë¶„ì„ ë° ë‹µë³€ ìƒì„± í´ë˜ìŠ¤ (ì•ˆì „í™”)"""
    
    def __init__(self, document_processor):
        self.document_processor = document_processor
        
        self.greeting_patterns = [
            'ì•ˆë…•', 'hi', 'hello', 'ì•ˆë…•í•˜ì„¸ìš”', 'ì²˜ìŒ', 'ì‹œì‘'
        ]
        
        self.thanks_patterns = [
            'ê°ì‚¬', 'ê³ ë§ˆì›Œ', 'ê³ ë§™', 'thank', 'ë„ì›€'
        ]
    
    def is_greeting(self, question):
        """ì¸ì‚¬ë§ì¸ì§€ í™•ì¸"""
        question_lower = question.lower()
        return any(pattern in question_lower for pattern in self.greeting_patterns)
    
    def is_thanks(self, question):
        """ê°ì‚¬ ì¸ì‚¬ì¸ì§€ í™•ì¸"""
        question_lower = question.lower()
        return any(pattern in question_lower for pattern in self.thanks_patterns)
    
    def generate_greeting_response(self):
        """ì¸ì‚¬ë§ ì‘ë‹µ ìƒì„±"""
        supported_formats = ", ".join(self.document_processor.supported_extensions.values())
        
        return f"""ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š

ì €ëŠ” ë‹¤ì¤‘ ë¬¸ì„œ í˜•ì‹ì„ ì§€ì›í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. 
ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

**ğŸ“ ì§€ì› íŒŒì¼ í˜•ì‹**:
â€¢ {supported_formats}

**ğŸ’¡ ì´ìš© ë°©ë²•**:
â€¢ ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ì„ ììœ ë¡­ê²Œ í•´ì£¼ì„¸ìš”
â€¢ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì¼ìˆ˜ë¡ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
â€¢ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì•ˆë‚´ë©ë‹ˆë‹¤

ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"""
    
    def generate_thanks_response(self):
        """ê°ì‚¬ ì¸ì‚¬ ì‘ë‹µ ìƒì„±"""
        return """ì²œë§Œì—ìš”! ğŸ˜Š

ë„ì›€ì´ ë˜ì…¨ë‹¤ë‹ˆ ê¸°ì©ë‹ˆë‹¤. 
ë‹¤ë¥¸ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”!

**ğŸ’¡ íŒ**: 
â€¢ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•˜ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆì–´ìš”"""
    
    def analyze_question(self, question):
        """ì§ˆë¬¸ ë¶„ì„ ë° ë‹µë³€ ìƒì„± (ì•ˆì „í™”)"""
        try:
            if not question or len(question.strip()) < 2:
                return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            question = question.strip()
            print(f"\n=== ì§ˆë¬¸ ë¶„ì„: {question} ===")
            
            # ì¸ì‚¬ë§ ì²˜ë¦¬
            if self.is_greeting(question):
                return self.generate_greeting_response()
            
            # ê°ì‚¬ ì¸ì‚¬ ì²˜ë¦¬
            if self.is_thanks(question):
                return self.generate_thanks_response()
            
            # ë¬¸ì„œ ê²€ìƒ‰
            search_results = self.document_processor.search_similar_documents(question, top_k=5, min_similarity=0.05)
            
            if not search_results:
                return self.generate_no_result_response_enhanced(question)
            
            # ë‹µë³€ ìƒì„±
            return self.generate_answer(question, search_results)
            
        except Exception as e:
            print(f"ì§ˆë¬¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def generate_no_result_response_enhanced(self, question):
        """ê²°ê³¼ê°€ ì—†ì„ ë•Œ í–¥ìƒëœ ì‘ë‹µ"""
        keywords = question.split()
        keyword_results = []
        
        for keyword in keywords:
            if len(keyword) > 1:
                for i, doc in enumerate(self.document_processor.documents):
                    if keyword.lower() in doc['content'].lower():
                        keyword_results.append({
                            'keyword': keyword,
                            'file_type': doc.get('file_type', 'Unknown'),
                            'content': doc['content'][:200] + "..."
                        })
                        break
        
        response = f'''**ğŸ“‹ "{question}"ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼**

ë²¡í„° ê²€ìƒ‰ì—ì„œëŠ” ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'''
        
        if keyword_results:
            response += f"\n\n**ğŸ” í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼:**\n"
            for result in keyword_results[:3]:
                response += f"\nâ€¢ **'{result['keyword']}'** ê´€ë ¨ ({result['file_type']}):\n{result['content']}\n"
        
        file_types = {}
        for doc in self.document_processor.documents:
            file_type = doc.get('file_type', 'Unknown')
            file_types[file_type] = file_types.get(file_type, 0) + 1
        
        file_stats = ", ".join([f"{ft}: {count}ê°œ" for ft, count in file_types.items()])
        
        response += f'''

**ğŸ’¡ ê²€ìƒ‰ ê°œì„  ì œì•ˆ:**
â€¢ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”
â€¢ ë” êµ¬ì²´ì ì´ê±°ë‚˜ ë” ì¼ë°˜ì ì¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”

**ğŸ“Š í˜„ì¬ ìƒíƒœ:**
â€¢ ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜: {len(self.document_processor.get_processed_files_info())}ê°œ
â€¢ ë¬¸ì„œ íƒ€ì…ë³„: {file_stats}
â€¢ ì´ ë¬¸ì„œ ì²­í¬: {len(self.document_processor.documents)}ê°œ

ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!'''
        
        return response
    
    def generate_answer(self, question, search_results):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        try:
            best_results = [r for r in search_results if r['similarity'] > 0.3]
            
            if not best_results:
                return self.generate_no_result_response_enhanced(question)
            
            answer = f"**ğŸ“‹ '{question}'ì— ëŒ€í•œ ë‹µë³€**\n\n"
            
            main_content = best_results[0]['content']
            answer += f"{main_content}\n\n"
            
            if len(best_results) > 1:
                answer += "**ğŸ“š ê´€ë ¨ ì¶”ê°€ ì •ë³´:**\n\n"
                for i, result in enumerate(best_results[1:3], 1):
                    content = result['content']
                    file_type = result['document'].get('file_type', 'Unknown')
                    if len(content) > 200:
                        content = content[:200] + "..."
                    answer += f"{i}. ({file_type}) {content}\n\n"
            
            sources_info = []
            for result in best_results:
                filename = result['document']['filename']
                file_type = result['document'].get('file_type', 'Unknown')
                sources_info.append(f"{filename} ({file_type})")
            
            unique_sources = list(dict.fromkeys(sources_info))
            answer += f"**ğŸ“– ì¶œì²˜**: {', '.join(unique_sources)}\n\n"
            
            avg_similarity = sum(r['similarity'] for r in best_results) / len(best_results)
            confidence = "ë†’ìŒ" if avg_similarity > 0.7 else "ë³´í†µ" if avg_similarity > 0.5 else "ë‚®ìŒ"
            answer += f"**ğŸ¯ ë‹µë³€ ì‹ ë¢°ë„**: {confidence} ({avg_similarity:.2f})"
            
            return answer
            
        except Exception as e:
            print(f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."